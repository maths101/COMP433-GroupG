{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Loading Data and Libaries </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Update dataset path\n",
    "dataset_path = r'D:\\Huron_Unlabeled_Data'\n",
    "\n",
    "# Set device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(('.png'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> SimCLR Dataset </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class SimCLR_Dataset(Dataset):\n",
    "    def __init__(self, image_files, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        try:\n",
    "            # Load the image and convert to RGB\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            # Apply transformations to generate two views\n",
    "            if self.transform:\n",
    "                img1 = self.transform(image)  # First view\n",
    "                img2 = self.transform(image)  # Second view\n",
    "\n",
    "                # Clamp the normalized images to a safe range [-1, 1]\n",
    "                img1 = torch.clamp(img1, min=-1.0, max=1.0)\n",
    "                img2 = torch.clamp(img2, min=-1.0, max=1.0)\n",
    "                \n",
    "            else:\n",
    "                img1, img2 = image, image  # No transformations applied\n",
    "            \n",
    "            return img1, img2\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return torch.zeros(3, 512, 512), torch.zeros(3, 512, 512)  # Return zeros if error occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Apply Transformations to prep Data for SSL encoder </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.8786, 0.8474, 0.8732]\n",
    "std = [0.2504, 0.2687, 0.2513]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Init Dataloader </H2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimCLR_Dataset(image_files, transform=transform)\n",
    "# Create DataLoader for SimCLR\n",
    "loader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Resnet-34 Encoder Class </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet34_SimCLR(nn.Module):\n",
    "    def __init__(self, feature_dim=256):  # Increase feature dimension\n",
    "        super(ResNet34_SimCLR, self).__init__()\n",
    "\n",
    "        # Load ResNet-34 model with pretrained weights\n",
    "        self.encoder = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Remove the final fully connected layer\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-1])\n",
    "\n",
    "        # Add a deeper SimCLR projection head (3-layer MLP)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(512, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),  # Larger final embedding\n",
    "            nn.BatchNorm1d(512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # Forward pass through the projection head\n",
    "        projections = self.projection_head(features)\n",
    "        return projections\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> NT-Xent Loss Function </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5, eps=1e-8):\n",
    "    batch_size = z_i.size(0)\n",
    "    \n",
    "    # Concatenate and normalize projections\n",
    "    z = torch.cat([z_i, z_j], dim=0)\n",
    "    z = F.normalize(z, dim=1, p=2)\n",
    "\n",
    "    # Compute similarity matrix with stability adjustments\n",
    "    similarity_matrix = torch.matmul(z, z.T) / temperature\n",
    "\n",
    "    # Avoid in-place operation for stability\n",
    "    similarity_matrix = torch.exp(similarity_matrix - similarity_matrix.max(dim=1, keepdim=True)[0].detach())\n",
    "    similarity_matrix = similarity_matrix / (similarity_matrix.sum(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    # Create labels for positive pairs\n",
    "    labels = torch.arange(batch_size).repeat(2).to(z.device)\n",
    "\n",
    "    # Mask self-similarity\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, 0)\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/100] started.\n",
      "Current Temperature: 0.0500\n",
      "Batch 1/128: Loss = 5.545173645019531\n",
      "Batch 2/128: Loss = 5.545180320739746\n",
      "Batch 3/128: Loss = 5.5451579093933105\n",
      "Batch 4/128: Loss = 5.545173645019531\n",
      "Batch 5/128: Loss = 5.543921947479248\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m temperature \u001b[39m=\u001b[39m adjust_temperature(epoch, epochs, initial_temp, max_temp)\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurrent Temperature: \u001b[39m\u001b[39m{\u001b[39;00mtemperature\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfor\u001b[39;49;00m batch_idx, (img1, img2) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(loader):\n\u001b[0;32m     31\u001b[0m     img1, img2 \u001b[39m=\u001b[39;49m img1\u001b[39m.\u001b[39;49mto(device), img2\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     33\u001b[0m     \u001b[39m# Debug: Check for NaN/Inf in input images\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[29], line 17\u001b[0m, in \u001b[0;36mSimCLR_Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     14\u001b[0m img_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_files[idx]\n\u001b[0;32m     15\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[39m# Load the image and convert to RGB\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(img_path)\u001b[39m.\u001b[39;49mconvert(\u001b[39m'\u001b[39;49m\u001b[39mRGB\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     19\u001b[0m     \u001b[39m# Apply transformations to generate two views\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\PIL\\Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    990\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mBGR;15\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBGR;16\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBGR;24\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    991\u001b[0m     deprecate(mode, \u001b[39m12\u001b[39m)\n\u001b[1;32m--> 993\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m    995\u001b[0m has_transparency \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtransparency\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\n\u001b[0;32m    996\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mode \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mP\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    997\u001b[0m     \u001b[39m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\PIL\\ImageFile.py:314\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_end()\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_exclusive_fp_after_loading:\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mclose()\n\u001b[0;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmap \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m LOAD_TRUNCATED_IMAGES \u001b[39mand\u001b[39;00m err_code \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    318\u001b[0m     \u001b[39m# still raised if decoder fails to return anything\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Set initial temperature and max temperature\n",
    "initial_temp = 0.05\n",
    "max_temp = 0.2\n",
    "epochs = 100\n",
    "\n",
    "# Define a function to increase temperature gradually\n",
    "def adjust_temperature(epoch, total_epochs, initial_temp, max_temp):\n",
    "    return initial_temp + (max_temp - initial_temp) * (epoch / total_epochs)\n",
    "\n",
    "# Set up model, optimizer, and data loader\n",
    "model = ResNet34_SimCLR(feature_dim=256).to(device)\n",
    "\n",
    "# Replace SGD with Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6)\n",
    "\n",
    "# Initialize GradScaler for mixed precision training\n",
    "scaler = GradScaler(\"cuda\")\n",
    "\n",
    "# Define gradient accumulation steps\n",
    "accumulation_steps = 4  # Number of batches to accumulate gradients before an optimizer step\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch [{epoch+1}/{epochs}] started.\")\n",
    "\n",
    "    # Adjust temperature for this epoch\n",
    "    temperature = adjust_temperature(epoch, epochs, initial_temp, max_temp)\n",
    "    print(f\"Current Temperature: {temperature:.4f}\")\n",
    "\n",
    "    for batch_idx, (img1, img2) in enumerate(loader):\n",
    "        img1, img2 = img1.to(device), img2.to(device)\n",
    "\n",
    "        # Debug: Check for NaN/Inf in input images\n",
    "        if torch.isnan(img1).any() or torch.isnan(img2).any():\n",
    "            print(f\"NaN detected in input images at Batch {batch_idx+1}!\")\n",
    "            continue\n",
    "        if torch.isinf(img1).any() or torch.isinf(img2).any():\n",
    "            print(f\"Inf detected in input images at Batch {batch_idx+1}!\")\n",
    "            continue\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with autocast(\"cuda\"):\n",
    "            z_i, z_j = model(img1), model(img2)\n",
    "\n",
    "            # Debug: Check for NaN/Inf in projections\n",
    "            if torch.isnan(z_i).any() or torch.isnan(z_j).any():\n",
    "                print(f\"NaN detected in projections at Batch {batch_idx+1}!\")\n",
    "                continue\n",
    "            if torch.isinf(z_i).any() or torch.isinf(z_j).any():\n",
    "                print(f\"Inf detected in projections at Batch {batch_idx+1}!\")\n",
    "                continue\n",
    "\n",
    "            # Calculate NT-Xent loss with updated temperature\n",
    "            loss = nt_xent_loss(z_i, z_j, temperature=temperature)\n",
    "\n",
    "            # Debug: Check for NaN/Inf in loss\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"Warning: Loss is NaN or Inf at Batch {batch_idx+1}!\")\n",
    "                continue\n",
    "\n",
    "            # Debug: Print loss value\n",
    "            print(f\"Batch {batch_idx+1}/{len(loader)}: Loss = {loss.item()}\")\n",
    "\n",
    "            # Normalize loss by accumulation steps\n",
    "            loss = loss / accumulation_steps\n",
    "\n",
    "        # Backpropagation with scaled loss\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Perform optimizer step and zero gradients every 'accumulation_steps'\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            # Debug: Check for NaN/Inf in gradients before clipping\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"NaN or Inf in gradients of {name} at Batch {batch_idx+1}!\")\n",
    "\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update model weights\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Accumulate total loss for average calculation\n",
    "        total_loss += loss.item() * accumulation_steps  # Multiply back to original scale\n",
    "\n",
    "    # Calculate average loss per epoch\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Avg Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
