{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Loading Data and Libaries </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Update dataset path\n",
    "dataset_path = r'D:\\Huron_Unlabeled_Data'\n",
    "\n",
    "# Set device for computation\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "image_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(('.png'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> SimCLR Dataset </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class SimCLR_Dataset(Dataset):\n",
    "    def __init__(self, image_files, transform=None):\n",
    "        self.image_files = image_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        try:\n",
    "            # Load the image and convert to RGB\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            # Apply transformations to generate two views\n",
    "            if self.transform:\n",
    "                img1 = self.transform(image)  # First view\n",
    "                img2 = self.transform(image)  # Second view\n",
    "\n",
    "                # Clamp the normalized images to a safe range [-1, 1]\n",
    "                img1 = torch.clamp(img1, min=-1.0, max=1.0)\n",
    "                img2 = torch.clamp(img2, min=-1.0, max=1.0)\n",
    "                \n",
    "            else:\n",
    "                img1, img2 = image, image  # No transformations applied\n",
    "            \n",
    "            return img1, img2\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return torch.zeros(3, 512, 512), torch.zeros(3, 512, 512)  # Return zeros if error occurs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Apply Transformations to prep Data for SSL encoder </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.8786, 0.8474, 0.8732]\n",
    "std = [0.2504, 0.2687, 0.2513]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Init Dataloader </H2>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimCLR_Dataset(image_files, transform=transform)\n",
    "# Create DataLoader for SimCLR\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> Resnet-34 Encoder Class </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet34_SimCLR(nn.Module):\n",
    "    def __init__(self, feature_dim=256):  # Increase feature dimension\n",
    "        super(ResNet34_SimCLR, self).__init__()\n",
    "\n",
    "        # Load ResNet-34 model without pre-trained weights\n",
    "        self.encoder = models.resnet34(weights=None)\n",
    "\n",
    "        # Remove the final fully connected layer\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-1])\n",
    "\n",
    "        # Add a deeper SimCLR projection head (3-layer MLP)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(512, 2048), \n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),  # Larger final embedding\n",
    "            nn.BatchNorm1d(512)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # Forward pass through the projection head\n",
    "        projections = self.projection_head(features)\n",
    "        return projections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> NT-Xent Loss Function </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5, eps=1e-8):\n",
    "    batch_size = z_i.size(0)\n",
    "    \n",
    "    # Concatenate and normalize projections\n",
    "    z = torch.cat([z_i, z_j], dim=0)\n",
    "    z = F.normalize(z, dim=1, p=2)\n",
    "\n",
    "    # Compute similarity matrix with stability adjustments\n",
    "    similarity_matrix = torch.matmul(z, z.T) / temperature\n",
    "\n",
    "    # Avoid in-place operation for stability\n",
    "    similarity_matrix = torch.exp(similarity_matrix - similarity_matrix.max(dim=1, keepdim=True)[0].detach())\n",
    "    similarity_matrix = similarity_matrix / (similarity_matrix.sum(dim=1, keepdim=True) + eps)\n",
    "\n",
    "    # Create labels for positive pairs\n",
    "    labels = torch.arange(batch_size).repeat(2).to(z.device)\n",
    "\n",
    "    # Mask self-similarity\n",
    "    mask = torch.eye(2 * batch_size, dtype=torch.bool).to(z.device)\n",
    "    similarity_matrix = similarity_matrix.masked_fill(mask, 0)\n",
    "\n",
    "    # Cross-entropy loss\n",
    "    loss = F.cross_entropy(similarity_matrix, labels)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/100] started.\n",
      "Batch 1/511: z_i min=-4.516993999481201, max=4.56895112991333, mean=-1.04046193882823e-09\n",
      "Batch 1/511: z_j min=-4.824975490570068, max=4.638307094573975, mean=-2.726665115915239e-09\n",
      "Batch 1/511: Loss = 4.159465312957764\n",
      "Batch 2/511: z_i min=-4.627386569976807, max=4.422669887542725, mean=1.709850039333105e-09\n",
      "Batch 2/511: z_j min=-4.875001907348633, max=4.836909294128418, mean=3.092281986027956e-10\n",
      "Batch 2/511: Loss = 4.158914566040039\n",
      "Batch 3/511: z_i min=-5.0673418045043945, max=5.162352085113525, mean=2.8194335754960775e-09\n",
      "Batch 3/511: z_j min=-4.979772090911865, max=5.117416858673096, mean=-7.457856554538012e-10\n",
      "Batch 3/511: Loss = 4.159491539001465\n",
      "Batch 4/511: z_i min=-5.251002788543701, max=5.24645471572876, mean=-2.597516868263483e-09\n",
      "Batch 4/511: z_j min=-5.254082202911377, max=4.960810661315918, mean=-5.493347998708487e-10\n",
      "Batch 4/511: Loss = 4.160037517547607\n",
      "Batch 5/511: z_i min=-4.658447265625, max=4.8316731452941895, mean=6.330083124339581e-10\n",
      "Batch 5/511: z_j min=-4.901234149932861, max=4.88233757019043, mean=1.3169483281672e-09\n",
      "Batch 5/511: Loss = 4.158876895904541\n",
      "Batch 6/511: z_i min=-4.639216423034668, max=4.429832935333252, mean=7.312337402254343e-10\n",
      "Batch 6/511: z_j min=-4.3919243812561035, max=4.208220481872559, mean=-7.785274647176266e-10\n",
      "Batch 6/511: Loss = 4.159536838531494\n",
      "Batch 7/511: z_i min=-4.043031215667725, max=4.054946422576904, mean=8.731149137020111e-11\n",
      "Batch 7/511: z_j min=-4.941170692443848, max=4.77311897277832, mean=1.6152625903487206e-09\n",
      "Batch 7/511: Loss = 4.1589226722717285\n",
      "Batch 8/511: z_i min=-4.39890718460083, max=4.462889671325684, mean=1.4479155652225018e-09\n",
      "Batch 8/511: z_j min=-5.011293411254883, max=4.621388912200928, mean=-8.767528925091028e-10\n",
      "Batch 8/511: Loss = 4.15920352935791\n",
      "Batch 9/511: z_i min=-4.303644180297852, max=4.527044773101807, mean=-1.4861143426969647e-09\n",
      "Batch 9/511: z_j min=-5.063586711883545, max=4.996320724487305, mean=-1.0732037480920553e-09\n",
      "Batch 9/511: Loss = 4.1588640213012695\n",
      "Batch 10/511: z_i min=-4.867434978485107, max=5.224936485290527, mean=-1.9972503650933504e-09\n",
      "Batch 10/511: z_j min=-5.200182914733887, max=5.23000955581665, mean=-9.89530235528946e-10\n",
      "Batch 10/511: Loss = 4.158883571624756\n",
      "Batch 11/511: z_i min=-4.583311557769775, max=4.55035924911499, mean=2.1464074961841106e-10\n",
      "Batch 11/511: z_j min=-5.19678258895874, max=5.315113544464111, mean=4.18367562815547e-11\n",
      "Batch 11/511: Loss = 4.158898830413818\n",
      "Batch 12/511: z_i min=-5.339418411254883, max=5.257109642028809, mean=-2.815795596688986e-09\n",
      "Batch 12/511: z_j min=-5.2835211753845215, max=5.076195240020752, mean=-6.766640581190586e-10\n",
      "Batch 12/511: Loss = 4.159283638000488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Forward pass to get projections\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m z_i, z_j \u001b[39m=\u001b[39m model(img1), model(img2)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Debug: Check for NaN/Inf in projections\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(z_i)\u001b[39m.\u001b[39many() \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misnan(z_j)\u001b[39m.\u001b[39many():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[126], line 29\u001b[0m, in \u001b[0;36mResNet34_SimCLR.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[39m# Forward pass through the encoder\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     30\u001b[0m     features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mview(features\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[39m# Forward pass through the projection head\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\resnet.py:102\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(x)\n\u001b[1;32m--> 102\u001b[0m out \u001b[39m+\u001b[39;49m\u001b[39m=\u001b[39;49m identity\n\u001b[0;32m    103\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\fx\\traceback.py:72\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m [current_meta\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstack_trace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m     70\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[39m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m traceback\u001b[39m.\u001b[39mformat_list(traceback\u001b[39m.\u001b[39;49mextract_stack()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\traceback.py:232\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[1;32m--> 232\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m    233\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[0;32m    234\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\traceback.py:395\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[39mfor\u001b[39;00m f, lineno \u001b[39min\u001b[39;00m frame_gen:\n\u001b[0;32m    393\u001b[0m         \u001b[39myield\u001b[39;00m f, (lineno, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 395\u001b[0m \u001b[39mreturn\u001b[39;00m klass\u001b[39m.\u001b[39;49m_extract_from_extended_frame_gen(\n\u001b[0;32m    396\u001b[0m     extended_frame_gen(), limit\u001b[39m=\u001b[39;49mlimit, lookup_lines\u001b[39m=\u001b[39;49mlookup_lines,\n\u001b[0;32m    397\u001b[0m     capture_locals\u001b[39m=\u001b[39;49mcapture_locals)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\traceback.py:434\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    430\u001b[0m     result\u001b[39m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    431\u001b[0m         filename, lineno, name, lookup_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39mf_locals,\n\u001b[0;32m    432\u001b[0m         end_lineno\u001b[39m=\u001b[39mend_lineno, colno\u001b[39m=\u001b[39mcolno, end_colno\u001b[39m=\u001b[39mend_colno))\n\u001b[0;32m    433\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m fnames:\n\u001b[1;32m--> 434\u001b[0m     linecache\u001b[39m.\u001b[39;49mcheckcache(filename)\n\u001b[0;32m    435\u001b[0m \u001b[39m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2032.0_x64__qbz5n2kfra8p0\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m   \u001b[39m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     74\u001b[0m     cache\u001b[39m.\u001b[39mpop(filename, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set initial temperature and max temperature\n",
    "initial_temp = 0.05\n",
    "max_temp = 0.2\n",
    "epochs = 100\n",
    "\n",
    "# Define a function to increase temperature gradually\n",
    "def adjust_temperature(epoch, total_epochs, initial_temp, max_temp):\n",
    "    return initial_temp + (max_temp - initial_temp) * (epoch / total_epochs)\n",
    "\n",
    "# Set up model, optimizer, and data loader\n",
    "model = ResNet34_SimCLR(feature_dim=128).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.000025, momentum=0.9, weight_decay=1e-6)\n",
    "\n",
    "# Define gradient accumulation steps\n",
    "accumulation_steps = 4  # Number of batches to accumulate gradients before an optimizer step\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(f\"\\nEpoch [{epoch+1}/{epochs}] started.\")\n",
    "\n",
    "    # Adjust temperature for this epoch\n",
    "    temperature = adjust_temperature(epoch, epochs, initial_temp, max_temp)\n",
    "    print(f\"Current Temperature: {temperature:.4f}\")\n",
    "\n",
    "    for batch_idx, (img1, img2) in enumerate(loader):\n",
    "        img1, img2 = img1.to(device), img2.to(device)\n",
    "\n",
    "        # Debug: Check for NaN/Inf in input images\n",
    "        if torch.isnan(img1).any() or torch.isnan(img2).any():\n",
    "            print(f\"NaN detected in input images at Batch {batch_idx+1}!\")\n",
    "            continue\n",
    "        if torch.isinf(img1).any() or torch.isinf(img2).any():\n",
    "            print(f\"Inf detected in input images at Batch {batch_idx+1}!\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass to get projections\n",
    "        z_i, z_j = model(img1), model(img2)\n",
    "\n",
    "        # Debug: Check for NaN/Inf in projections\n",
    "        if torch.isnan(z_i).any() or torch.isnan(z_j).any():\n",
    "            print(f\"NaN detected in projections at Batch {batch_idx+1}!\")\n",
    "            continue\n",
    "        if torch.isinf(z_i).any() or torch.isinf(z_j).any():\n",
    "            print(f\"Inf detected in projections at Batch {batch_idx+1}!\")\n",
    "            continue\n",
    "\n",
    "        # Calculate NT-Xent loss with updated temperature\n",
    "        loss = nt_xent_loss(z_i, z_j, temperature=temperature)\n",
    "\n",
    "        # Debug: Check for NaN/Inf in loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Warning: Loss is NaN or Inf at Batch {batch_idx+1}!\")\n",
    "            continue\n",
    "\n",
    "        # Debug: Print loss value\n",
    "        print(f\"Batch {batch_idx+1}/{len(loader)}: Loss = {loss.item()}\")\n",
    "\n",
    "        # Normalize loss by accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimizer step and zero gradients every 'accumulation_steps'\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            # Debug: Check for NaN/Inf in gradients before clipping\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"NaN or Inf in gradients of {name} at Batch {batch_idx+1}!\")\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Update model weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Accumulate total loss for average calculation\n",
    "        total_loss += loss.item() * accumulation_steps  # Multiply back to original scale\n",
    "\n",
    "    # Calculate average loss per epoch\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] completed. Avg Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
